---
layout: post
title: On Interpretable Language Modeling
---

How do we acquire and use language? How do babies learn word meanings, word compositions and grammar? How can children produce novel phrases or sentences that they themselves have never heard? And how is language related to intelligence? These are some of the questions people have been interested in for many years. Some thought reverse engineering the cellular mechanism and the structure of the brain will lead them to the right answer, so they became neuroscientists. Some decided to accept the brain as a black box, and study it behaviorally by looking at how it reacts to a given input in certain situations, we call them psychologists. We also have some people who are building artificial black boxes (neural network language models) to mimic the behavior of the biological black boxes, then trying to reverse engineer them in order to understand both the artificial and the biological ones. This used to be called BERTology, now you can also call it mechanistic interpretability research. I'm in favor of a fourth type of approach: building interpretable models from the ground up. 

But first of all, what even is modeling? In my opinion it's simply the act of replacing the subject of interest with something else that is similar in quality, appearance or behavior, but smaller, cheaper, more manageable and more controllable. For example, we may use aircraft models to study the aerodynamics of real airplanes, use weather models to predict the future weather, or use fashion models and mannequins to display how a piece of clothing may look on customers' bodies. Similarly, we can use a **model of language** to study the nature of human language, cognition and intelligence. Importantly, modeling has to be a cyclic process: we use our initial theory to build a model, we use the model to test our hypotheses and propose better theories, and use that to design even better models. 

<img class="centered bg-white" src="https://raw.githubusercontent.com/DeMoriarty/DeMoriarty.github.io/master/images/modelig_cycle.png"/>  

Large language models (LLMs) are extremely good at ... language modeling, we must give them credit. But how much can they contribute to understanding language and the human mind? Well, not a lot. One of the biggest problems with LLMs is that they are huge black-boxes, which means they are not inherently interpretable. BERTologists, or mechanistic interpretability researchers, are trying to find out whether LLMs are actually trying to approximate some discrete, mechanistic algorithms in the high-dimensional space of the language model's hidden states. There are some interesting findings, but in general, this approach is too opportunistic, not very generalizable (findings from one model may not apply to another one), and the sheer number of parameters and dimensions makes discovering meaningful patterns/algorithms difficult. Interpretability is one of the most important problems in AI research, not only because we want our AI systems to be safe, reliable and robust, but also because we simply can't improve an AI without knowing what it's exactly doing. Remember that modeling is a cyclic process, when modeling doesn't provide better understanding, we can't make better models. In fact, most of the recent advancements in language modeling come from one ridiculously simple idea: more is better. 

You need to understand how logic gates work in order to build a circuit that multiplies floating point numbers, you need to know how semiconductors work to build logic gates, and you need to understand physics to make better semiconductors. If logic gates were to grow on trees, we might still be able to use them to build powerful computers, by mass planting the "logic gate trees", but we would eventually hit a wall unless we understand the underlying mechanism of those "biological logic gates" and manage to improve them.

There is a lot of evidence that our brain relies on statistics when learning a language. And statistical learning should be one of the main pillar of any linguistic theory. But being statistical doesn't justify the absence of interpretability. I believe that statistical language models can totally be interpretable, as long as they are built gradually within the modeling-understanding cycle.

In future blog posts, I will talk about some of my own research in computational linguistics. Some of the topics are: similarity based generalization, word sense induction, distributional composition, grammar induction, etc. 
