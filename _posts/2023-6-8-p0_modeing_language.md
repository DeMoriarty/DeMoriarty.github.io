---
layout: post
title: Modeling Language (0) &#58; Introduction
---

How do we acquire and use language? how do babies learn word meanings, word compositions and grammar? how can children produce phrases or sentences that they themselves have never heard? and how can a computer do all of the above?

People have been curious about the mystery of language, especially its relation to human intelligence, for many decades, and there is a great amount of research trying to seek answer to these questions scientifically from various different perspectives, such as neuroscience and psychology. Great amount of progress has been made, many theories have been proposed, but many of these questions still remain open. We're closer to the truth, but still not there yet.

In this series of blog posts, we will explore some of the concepts and problems in computational modelling of natural language. In each topic, I will first give a broad overview of the literature, discuss the intuition behind the existing methods, and the problems that comes with them. Lastly, I will talk about the research I have done, or some ideas I have on each topic. Basically, they will be structured very loosely like research papers, and I will use this blog to publish my research, and express my opinions.   

First of all, what even is modeling? in my opinion it's simply the act of replacing the subject of interest with something else that is similar in quality, appearance or behavior, but smaller, cheaper, more manageable and controllable. For example, we may use aircraft models to study the aerodynamics of real airplanes, use weather models to predict the future weather, or use fashion models and mannequins to display how a piece clothing may look on customers' body. Similarly, we can use a **model of the language** to study human language, cognition and intelligence. Importantly, modeling is a cyclic process: we use our initial theory to build a model, we use the model to test our hypotheses and propose better theories, and use that to design even better models. 

<img class="centered bg-white" src="https://raw.githubusercontent.com/DeMoriarty/DeMoriarty.github.io/master/images/modelig_cycle.png"/>  

It may sound like this series will be all about Large Language Models (LLM), since that's probably the first thing come to people's mind these days when talking about modeling language. LLMs are extremely good at... language modeling, we must give them credit. But how much can they contribute to understanding of language and human mind? Well, not a lot. One of the biggest problems of LLMs is that they are huge black-boxes, which means they are not inherently interpretable. There are ongoing attempts at demystifying LLM black-boxes, an emerging subfield called mechanistic interpretablity, where researchers try to find out whether LLMs are actually trying to approximate some descrete, mechanistic algorithms in the high dimensional space of model's hidden states. There are some interesting findings, but in general this approach is too opportunistic, findings from one model may not apply to another one, and the sheer number of parameters and dimensions makes discovering meaningful patterns very difficult. Interpretability in my opinion is one of the most important problems in AI research, not only because we want our AI systems to be safe, reliable and robust, but also because we simply can't improve an AI without knowing what it's exactly doing. Remember that modeling is a cyclic process, when modeling doesn't provide better understanding, we can't make a better model. In fact, most of the recent advancement in language modeling research come from one ridiculously simple idea: more is better.

Our brain relies on statistics when learning a language. And statistical learning should be one of the main building block of any linguistic theory. But that doesn't justify the absence of interpretability. I believe that a statistical model of language can totally be interpretable, as long as it's built gradually within the modeling-understanding cycle.
