---
layout: post
title: Modifying Custom Matmul CUDA Kernel
---

I started to learn CUDA at the end of last year, and started writing matrix multiplication kernels as a learning project. After some struggles, I made it working, but then got really disappointed when I saw it's 10x slower than cuBLAS GEMM (I don't know what I was expecting to be honest). I've tried lots of open sourced matmul kernels on github, but the best one I found was still about 5 times slower (some of them were optimized towards older architectures). After few months of trial and error, my matmul kernel finally has comparable speed to cuBLAS GEMM.  
<details markdown="1">  
  <summary markdown='span'>
    <b>Click here to see comparison:</b>
  </summary>  
  
### Square matrices: batch size = 1, m = n = k
![bmm_1](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/bmm_A%5B1%2CN%2CN%5D%20B%5B1%2CN%2CN%5D.png)  

### Batch size = 128, k = 128, varying m, n (m = n)
![bmm_2](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/bmm_A%5B128%2CN%2C128%5D%20B%5B128%2C128%2CN%5D.png)  
</details>

Maybe I will write a tutorial on matmul kernel optimization later, in this post I mainly want to talk about some interesting modifications that can be done on a matmul kernel. 
- [Fused Reduce Matmul](#fused-reduce-matmul)
- [Topk Search](#topk-search)
- [Masked Batch Matmul](#masked-bmm)
- [Fused Attention](#fused-attention)  
 
## Fused Reduce Matmul
Sometimes we want to apply reduction right after a matrix multiplication:

```python
C = torch.bmm(A, B)
maxC, argmaxC = torch.max(C, dim=1)
sumC = torch.sum(C, dim=2)
argminC = torch.argmin(C, dim=1)
```

Normally there isn't a problem with doing this, however, when optimizing my implementation of K-means clustering algorithm, this become an issue. One of the main steps of K-means algorithm is computing the pairwise distance between every data point and every centroid (cluster center), and get index of the closest cluster for each data point (argmin).   

When the number of data points (n_data) and the number of clusters (n_clusters) are very large, this step will produce a huge (n_data x n_clusters) matrix that might not fit into the GPU memory (imagine a 1,000,000 x 10,000 fp32 matrix)   

One workaround is to split this step into multiple tiny steps: in each step only compute the distance between a subset of data points and centroids (let's say 10,000 x 10,000), then asign each data point to the closest cluster, and repeat.  

A better solution could be to **fuse argmin into the matmul kernel**. Advantages of doing this are:  
1. No need to create huge pairwise distance matrix if the result of argmin (or any type of reduction) is all we care about  
2. No need for a loop  
3. Possibily faster because of fewer memory loads.  

I won't go into details, if you are interested, you can read [implementation details](% link _posts/2021-5-17-MinBMM.md %), and check the [source code](https://github.com/DeMoriarty/TorchPQ/blob/main/torchpq/kmeans/kernels/MaxSimKernel.cu) (temporary)

I compared the new MinBMM kernel with 2 things:
```python
values, indices = torch.min(torch.bmm(a, b), dim = dim)
```
and
```python
values, indices = torch.min(custom_bmm(a, b), dim = dim)
```
And here are the results:
<details markdown="1">
  <summary markdown="span">
    <b>Click here to show plots</b>
  </summary>
 
### batch_size = 1, k = 16, varying m, n (m = n)  
![minbmm_1](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/min_bmm_A%5B1%2CN%2C16%5D%20B%5B1%2C16%2CN%5D.png)  

### batch_size = 1, k = 64, varying m, n (m = n)
![minbmm_2](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/min_bmm_A%5B1%2CN%2C64%5D%20B%5B1%2C64%2CN%5D.png)  

### batch_size = 1, k = 256, varying m, n (m = n)
![minbmm_3](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/min_bmm_A%5B1%2CN%2C256%5D%20B%5B1%2C256%2CN%5D.png)  

### batch_size = 64, k = 64, varying m, n (m = n)
![minbmm_4](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/min_bmm_A%5B64%2CN%2C64%5D%20B%5B64%2C64%2CN%5D.png)  
</details>
From the benchmark results we can see that fused MinBMM kernel is much faster than calling *min* and *bmm* kernels seperately, especially when k is smaller. The speedup seems to decay as k growth larger. Another advantage of MinBMM is the required memory is much smaller than other methods, the inputs can be (1,000,000 x k) matrices, and we still don't need to be worried about going out of memory. 

## Topk Search
Nearest Neighbor Search (NNS) and Maximum Inner Product Search (MIPS) are some of the most important components of information retrieval and data science. Sometimes its required to search in million or billions of vectors with thousands of queries within seconds.  

The naive approach is to calculate pairwise distance matrix (or dot product) of dataset vectors and query vectors first, then apply topk search, this is also called "bruteforce search". Here is an example in pytorch:

```python
n_data = 1_000_000
n_query = 1000
d_vector = 256
a = torch.randn(n_data, d_vector)
b = torch.randn(n_queries, d_vector)
c = a.pow(2).sum(dim=-1)[:, None] + b.pow(2).sum(dim=-1)[None] - 2 * a @ b.T # squared L2 distance
# c = a @ b.T # inner product

topk_val, topk_idx = torch.topk(c, dim=0, k=128)
```
Doing this is OK in smaller scale, but as *n_data* grows larger, bruteforce search will get more and more expensive. It also has the same problem as "matmul & reduce" that I talked about in previous section: the pairwise distance (or dot product) matrix may not fit into GPU ram.  

There are some approximate nearest neighbor search algorithms that try to solve the performance problem, such as HNSW, PQ and LHS. I have implemented some variations of PQ algorithm earlier this year, if you're interested you can [take a look](https://github.com/DeMoriarty/TorchPQ).  

In this post I'm not going to focus on approximate methods, instead I will try to speed up "bruteforce search" by **fusing topk search into matmul kernel**. Like in MinBMM kernel, all there needs to be changed is the part where we store calculated results to global memory (DRAM). In MinBMM, after matrix multiplication, instead of writing 128 x 128 block of C matrix to DRAM, we first did an in-block reduction (128 x 128) --> (128), then a global reduction using atomic operations. Similarly, in TopkBMM kernel, we will first perform an in-block bitonic sort along one of the two dimentions of C matrix, and a global sort after. We also need to have mutex (mutual exclusion) in order to avoid racing condition (multiple threads trying to write to the same global memory location). Maybe I will write another post to explain the details if anyone is interested. the source code can be found [here](https://github.com/DeMoriarty/custom_matmul_kernels) (temporary)  

I compared it with ```torch.topk(torch.bmm(a, b), k=128)```, and here are the results:  

<details markup="1">  
  <summary markup="span">  
    <b>Click here to show plots</b>    
  </summary>  

A is a (m x k) matrix, B is a (k x n) matrix , m is the number of data points, n is the number of query vectors, k is the vector dimentionality.  
  
### n = 1024, k = 64, varying m:  
![topk_bmm_1](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/topk_bmm_A%5B1%2CN%2C64%5D%20B%5B1%2C64%2C1024%5D_semilogx.png)  
  
### n = 1024, k = 256, varying m:  
![topk_bmm_2](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/topk_bmm_A%5B1%2CN%2C256%5D%20B%5B1%2C256%2C1024%5D_semilogx.png)  
  
### n = 1024, k = 1024, varying m:  
![topk_bmm_3](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/topk_bmm_A%5B1%2CN%2C1024%5D%20B%5B1%2C1024%2C1024%5D_semilogx.png)  
</details>  

TopkBMM kernel is extremely fast! it's up to 10x faster than naive "bruteforce search" when number of data points are close to a million. To be honest I wasn't expecting this. And this is not the end! we can further improve its speed by switching to half type (float16), it's also possible to use tensor cores of more recent nvidia GPUs to accelerate the matrix multiplication.

One limitation is it can search at most top 128 nearest neighbors. In order to search for more than 128 neighbors, we can do it iteratively, for example, first search the top 128 neighbors, then 128 ~ 256, then 256 ~ 384, and so on.

### L2 distance
For L2 distance, all we need to change is a single line in *thread_matmul*:
```c
cCache[m].val[n] = fmaf(a, b, cCache[m].val[n]);
```  
to
```c
float dif = a - b;
cCache[m].val[n] = fmaf(-dif, dif, cCache[m].val[n]);
```  
Now the BMM kernel will calculate **negative squared L2 distance** instead of dot product. It's negative because only then large values would mean 2 points are close to each other. We also don't calculate square root since it doesn't affect the order of nearest neighbors.

## Masked BMM
Masked BMM (Batch Matrix Multiplication) means "hiding" some portion of the output of BMM using a binary mask. One use case of this operation is the self-attention layer of GPT style language models. After computing dot product of queries and keys, we fill the upper triangular half of the output matrix with -âˆž. Here is how it's normally done in pytorch:  
```python
# shape of keys: [b, m, k]
# shape of values: [b, k, n]
# shape of mask: [m, n]
dot = torch.bmm(keys, queries)
dot.masked_fill_(mask = mask, value = float("-inf") )
```  
### Mask pattern

<img src="https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/mask2.png" width="30%" />

If we fuse masked_fill into the matmul kernel, we not only can get rid of masked_fill, but the matmul kernel itself can take advantage of the sparcity given by the mask (by not computing the masked part), reducing computation even further.  

The matmul kernel splits the output matrix C into a grid of 128 x 128 submatrices, each submatrix is assigned to a thread block. Each thread block consists of 256 threads, and each thread computes an 8 x 8 block of the 128 x 128 submatrix.  

First we need to do some preprocessing. We need 3 different levels of mask: block mask, thread mask and element mask. Block mask indicates which thread blocks should be ignored, and same goes for thread mask and element mask.  
```python
m, n = mask.shape
assert m % 128 == 0
assert n % 128 == 0

element_mask = mask
thread_mask = mask.view( m/8, 8, n/8, 8)
thread_mask = thread_mask.sum(dim=1).sum(dim=3)
block_mask = mask.view(m/128, 128, n/128, 128)
block_mask = block_mask.sum(dim=1).sum(dim=3)
```  
for simplicity, here we assume both m and n are fully divisible by 128.  

We input all 3 masks to the MBMM kernel:  
```cpp
extern "C"
__global__ void mbmm_nn(
  const float* __restrict__ A,
  const float* __restrict__ B,
  float* __restrict__ C,
  const uint8_t* __restrict__ BlockMask,   //
  const uint8_t* __restrict__ ThreadMask,  //
  const uint8_t* __restrict__ ElementMask, //
  int M, int N, int K
){
  ...
}
```
At the very start of the kernel, all threads within a thread block will read the block mask value, if it is 0, all threads within that thread block will exit.  (No memory store/loads, no arithmetic ops.)  
```cpp
int bN = (N + 128 - 1) / 128;
uint8_t block_mask = BlockMask[(blockIdx.y)*bN + (blockIdx.x)];
if (block_mask == 0){
  return;
}
```  

Each thread will also read a unique thread mask value:
```cpp
int vx = threadIdx.x % 16;
int vy = threadIdx.x / 16;
uint8_t thread_mask = ThreadMask[(blockIdx.y*16 + vy)*tN + (blockIdx.x*16 + vx) ];
```  
if it's 0, then that thread will skip the thread matmul step. 
```cpp
// main loop
for (int i=0; i<nItr; i++)
{
  ...
  
  if (thread_mask != 0){
    thread_matmul(aSM, bSM, cCache, vx, vy);
  }
  __syncthreads();
}
```  
However, all threads still participate in loading matrix A and B from global memory.

At the end of the kernel, before storing cached results to C, each thread will check its thread mask value once again, if it's between 0 and 64, that thread will mask the cached result using element mask.  
```c
if (0 < thread_mask < 64){
  mask_cCache(cCache, ElementMask, gStartx, gStarty, vx, vy, bid, M, N);
}
write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);
```  

This is similar to the idea of [Block Sparse](https://github.com/openai/blocksparse). The difference is we didn't design an entire new kernel for it, we just modified the existing matmul kernel.  

Now let's see how the new MBMM kernel performs compared to the original custom BMM + masked_fill and torch.bmm (cuBLAS) + masked_fill:
<details markdown=1>  
  <summary>  
    <b>Click to show plots</b>
  </summary>  

### 1. batch size = 128, m = n = 1024, varying k:
![mbmm_1](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/mbmm_A%5B128%2C1024%2CX%5D%20B%5B128%2CX%2C1024%5D.png)  

### 2. batch size = 128, k = 64, varying m, n (m = n):
![mbmm_2](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/mbmm_A%5B128%2CX%2C64%5D%20B%5B128%2C64%2CX%5D.png)  

### 3. m = n = 1024, k = 128, varying batch size:
![mbmm_3](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/mbmm_A%5BX%2C1024%2C128%5D%20B%5BX%2C128%2C1024%5D.png)  

</details>
We can see that, the new MBMM kernel has roughly 2 times the speed of the original BMM kernel + masked_fill, and 1.2 ~ 1.4 times the speed of torch.bmm + masked_fill.

## Fused Attention
content  
