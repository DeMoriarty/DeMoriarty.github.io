---
layout: post
title: Modifying Custom Matmul CUDA Kernel
---

I started to learn CUDA at the end of last year, and started writing matrix multiplication kernels as a learning project. After some struggles, I made it working, but then got really disappointed when I saw it's 10x slower than cuBLAS GEMM (I don't know what I was expecting to be honest). I've tried lots of open sourced matmul kernels on github, but the best one I found was still about 5 times slower (some of them were optimized towards older architectures). After few months of trial and error, my matmul kernel finally has comparable speed to cuBLAS GEMM.  
here are some comparisons:  
![bmm_1](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/bmm_1.png "BMM1")  

Maybe I will write a tutorial on matmul kernel optimization later, in this post I mainly want to talk about some interesting modifications that can be done on a matmul kernel.
- [Fused Reduce Matmul](#fused-reduce-matmul)
- [Nearest Neighbor Search](#nearest-neighbor-search)
- [Masked Batch Matmul](#masked-bmm)
- [Fused Attention](#fused-attention)  

## Fused Reduce Matmul
content  

## Nearest Neighbor Search
content  

## Masked BMM
Masked BMM (Batch Matrix Multiplication) means "hiding" some portion of the output of BMM using a binary mask. One example of this is the self-attention layer of GPT style language models. After computing dot product of queries and keys, we fill the upper triangular half of the output matrix with -âˆž. Here is how it's normally done in pytorch:  
```python
# shape of keys: [..., m, k]
# shape of values: [..., k, n]
# shape of mask: [m, n]
dot = torch.bmm(keys, queries)
dot.masked_fill_(mask = mask, value = float("-inf") )
```  
### Mask pattern

<img src="https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/mask2.png" width="50%" />

If we fuse masked_fill into the matmul kernel, we not only can get rid of masked_fill, but the matmul kernel itself can take advantage of the sparcity given by the mask (by not computing the masked part), reducing computation even further.  

The matmul kernel splits the output matrix C into a grid of 128 x 128 submatrices, each submatrix is assigned to a thread block. Each thread block consists of 256 threads, and each thread computes an 8 x 8 block of the 128 x 128 submatrix.  

Now we can start implementing it. First we need to do some preprocessing. We need 3 different levels of mask: block mask, thread mask and element mask. Block mask indicates which blocks should be ignored, and same goes for thread mask and element mask.  
```python
element_mask = mask
m, n = mask.shape
thread_mask = mask.view( m/8, 8, n/8, 8)
thread_mask = thread_mask.sum(dim=1).sum(dim=3)
block_mask = mask.view(m/128, 128, n/128, 128)
block_mask = block_mask.sum(dim=1).sum(dim=3)
```  
here we assume both m and n are fully divisible by 128 for simplicity.  
We input all 3 masks to the MBMM kernel.  
At the very start of the kernel, all threads within a block will read the block mask value, if it is 0, all threads within that block will exit.  (No memory store/load, no arithmetic ops.)  

Each thread will also read a unique thread mask value, if it's 0, then that thread will skip the thread matmul step. However, all threads still participate in loading matrix A and B from global memory. (No arithmetic ops.)  

At the end of the kernel, before storing cached results to C, each thread will check its thread mask value once again, if it's between 0 and 64, that thread will mask the cached result using element mask.  

This is similar to the idea of [Block Sparse](https://github.com/openai/blocksparse). The difference is we didn't design a whole new kernel for it, we just modified the existing matmul kernel.
## Fused Attention
content  
