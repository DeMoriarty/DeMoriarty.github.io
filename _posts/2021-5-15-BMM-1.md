---
layout: post
title: Things You Can Do With A Custom MatMul CUDA Kernel
---

I started to learn CUDA at the end of last year, and started writing matrix multiplication kernels as a learning project. I got it to work, but then got really disappointed when I see it's 10x slower than cuBLAS GEMM (I don't know what I was expecting to be honest). I've tried lots of open sourced matmul kernels on github, but the best one I found was still about 5 times slower (some of them were optimized towards older architectures). After few months of trial and error, my matmul kernel finally has comparable speed to cuBLAS GEMM. 
here are some benchmark results:  
![bmm_1](https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/bmm_1.png "Logo Title Text 1")  

<p float="left">
  <img src="https://raw.githubusercontent.com/DeMoriarty/custom_matmul_kernels/main/imgs/bmm_1.png" width="50%"/>  
</p>  


Matrix multiplication is one of the main building blocks of deep learning, and cuBLAS is what's behind most of the deep learning libraries such as tensorflow and pytorch (I realy thought they write their own CUDA kernels). cuBLAS GEMM (GEneral Matrix Multiplication) kernels are highly optimized and very efficient, it's not a surprise that they're the default (and only) option for most people. However, cuBLAS is not open sourced, when you
