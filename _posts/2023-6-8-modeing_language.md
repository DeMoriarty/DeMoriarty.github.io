---
layout: post
title: On Interpretable Language Modeling
---

How do we acquire and utilize language? How do infants learn word meanings, word compositions, and grammar? How do children produce new phrases or sentences they have never heard before? Furthermore, how does language relate to intelligence? These are questions that have captivated researchers for many years.

Some individuals believed that reverse engineer the cellular mechanisms and the structure of the brain would lead them to the answers. They became neuroscientists; Others accepted the brain as a black box and focused on behavioral studies, observing its reactions to specific inputs in various situations. They're known as psychologists. Additionally, there are those who try to reverse engineer artificial black boxes (neural network language models), in order to gain understanding of the mechanistic nature of language. This used to be called "BERTology", but now you can also call it mechanistic interpretability research. 

Personally, I strongly favor a fourth approach: building interpretable models from the ground up. This was once the standard in machine learning. Even in the domain of natural language processing (NLP), researchers used to leverage their understanding of natural language to model its various elements. Unfortunately, many of them lacked the right theories to support their models. Some became excessively focused on symbolic logic and formal grammar, disregarding a fundamental aspect of language processing: acquisition. Nowadays, the majority of NLP researchers have embraced neural networks because they consistently achieve State of the Art (SOTA) performance in nearly all benchmarks with minimal effort, albeit at the cost of interpretability.

But what does "modeling" actually mean? In my opinion, it is simply the act of substituting the subject of interest with something else that shares similar qualities, appearance, or behavior, but is smaller, more affordable, manageable, and controllable. For example, aircraft models are used to study the aerodynamics of real airplanes, weather models are employed to predict future conditions, and fashion models and mannequins display how clothing might appear on customers' bodies. Similarly, we can utilize a language model to investigate the nature of human language, cognition, and intelligence. Crucially, modeling must be a cyclical process: we utilize our existing theories to construct a model, use the model to test hypotheses and propose better theories, and then employ those theories to design even better models.

<img class="centered bg-white" src="https://raw.githubusercontent.com/DeMoriarty/DeMoriarty.github.io/master/images/modelig_cycle.png"/>  

Large language models (LLMs) excel at... language modeling, and they definitely deserve credit for that. However, their contribution to understanding language and the human mind has been limited. LLMs are black boxes, which means they are not inherently interpretable. Researchers specializing in BERTology, or mechanistic interpretability, are exploring whether LLMs attempt to approximate discrete, mechanistic algorithms within the high-dimensional space of the language model's hidden states. Some intriguing discoveries have been made, such as induction heads, emergent features and superposition, but this area of study is still at an early stage and progress is challenging.

Interpretability is a crucial challenge in AI research, not only because we aim for safe, reliable, and robust AI systems, but also because understanding exactly how they operate is essential for their improvement. It's important to remember that modeling is a cyclical process: without gaining a better understanding through modeling, we cannot develop better models. Interestingly, most recent advancements in language modeling can be attributed to one straightforward idea: more is better. Currently, the LLM communities are heavily focused on the topic of emergence, which refers to the phenomena of LLMs automagically acquiring various skills when the model size and dataset size reach a certain threshold, without being explicitly designed for them. While emergence is undoubtedly an intriguing research area, should we rely solely on it to make further progress in AI? Additionally, how reliable are these emergent capabilities anyways?

To illustrate the importance of understanding underlying mechanisms, let's consider the construction of a circuit that multiplies floating-point numbers. In order to achieve this, comprehension of logic gates is crucial. Similarly, to create logic gates, an understanding of semiconductor is necessary. Ultimately, to improve semiconductors, a deep comprehension of the underlying physics will be indispensable. Even if logic gates were to miraculously sprout from trees, allowing us to cultivate an abundance of "logic gate trees" to build computers, progress would inevitably stagnate unless we delve into the underlying mechanisms of these "biological logic gates" and succeed to enhance them.

There is ample evidence supporting the notion that our brains utilize statistical information during language acquisition. As a result, statistical learning should serve as a fundamental component of any linguistic theory. However, it is important to note that being statistical does not excuse the lack of interpretability. In my opinion, statistical language models can indeed be interpretable, provided they are constructed gradually within the modeling-understanding cycle.

In upcoming posts, I will delve into my own research in computational linguistics, covering various topics such as similarity-based generalization, word sense induction, distributional composition, grammar induction, and more.
