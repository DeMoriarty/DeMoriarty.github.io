---
layout: post
title: On Interpretable Language Modeling
---

How do we acquire and use language? How do babies learn word meanings, word compositions and grammar? How can children produce novel phrases or sentences that they themselves have never heard? And how is language related to intelligence? These are some of the questions people have been interested in for many years. Some thought reverse engineering the cellular mechanism and the structure of the brain will lead them to the right answer, so they became neuroscientists. Some decided to accept the brain as a black box, and study it behaviorally by looking at how it reacts to certain inputs in certain situations, and we call them psychologists. We also have some people who are building artificial black boxes (neural network language models) to mimic the behavior of biological black boxes, then trying to reverse engineer the former in order understand both the former and the latter. This used to be called BERTology, now you can also call it mechanistic interpretability research. 

Personally, I'm more a fan of a fourth type of approach: building interpretable models from the ground up. This used to be the norm in machine learning. Even in NLP, researchers used to use their understanding of natural language to model many aspects of it. But unfortunately many of them didn't have the right theories to support their models, some of them got too caught up on symbolic logic and formal grammar, and ignored one of the most fundamental aspects of language processing: acquisition. And nowadays, majority of NLP researchers gave in to neural networks, because that's what gets them the State of the Art (SOTA) in almost all benchmark with the least amount of effort.

What even is modeling? In my opinion it's simply the act of replacing the subject of interest with something else that is similar in quality, appearance or behavior, but smaller, cheaper, more manageable and more controllable. For example, we may use aircraft models to study the aerodynamics of real airplanes, use weather models to predict the future weather, or use fashion models and mannequins to display how a piece of clothing may look on customers' bodies. Similarly, we can use a **model of language** to study the nature of human language, cognition and intelligence. Importantly, modeling has to be a cyclic process: we use our initial theory to build a model, we use the model to test our hypotheses and propose better theories, and use that to design even better models. 

<img class="centered bg-white" src="https://raw.githubusercontent.com/DeMoriarty/DeMoriarty.github.io/master/images/modelig_cycle.png"/>  

Large language models (LLMs) are extremely good at ... language modeling, we must give them credit. But how much can they contribute to understanding of language and the human mind? Well, not a lot. One of the major issues with LLMs is that they are huge black-boxes, which means they are not inherently interpretable. BERTologists, or mechanistic interpretability researchers, are trying to find out whether LLMs are actually trying to approximate some discrete, mechanistic algorithms in the high-dimensional space of the language model's hidden states. There are some interesting findings, but in my opinion, this approach is too opportunistic, not very generalizable (findings from one model may not apply to another one), and the sheer number of parameters and dimensions makes discovering meaningful patterns/algorithms considerably difficult. 

Interpretability is one of the key problems in AI research, not only because we want our AI systems to be safe, reliable and robust, but also because we simply can't improve an AI system without knowing what it's exactly doing. Remember that modeling is a cyclic process, when modeling doesn't provide better understanding, we can't make better models. In fact, most of the recent advancements in language modeling come from one ridiculously simple idea: more is better. **Emergence** is one of the hottest topics in LLM communities at the moment, it refers to the phenomena of LLMs learning certain skills automagically when the size of the model and the dataset pass certain threshold, without ever being specifically designed to do so. It's definitely an interesting research topic on its own, but do we really want to rely on it?

You need to understand logic gates in order to build a circuit that multiplies floating point numbers, you need to know how semiconductors work to build logic gates, and you need to understand physics to make better semiconductors. If logic gates were to grow on trees, we might still be able to use them to build powerful computers, by mass planting the "logic gate trees", but the progress will eventually slow down unless we understand the underlying mechanism of those "biological logic gates" and manage to improve them.

There is a lot of evidence that our brain rely on statistics when learning a language. And statistical learning should be one of the main pillar of any linguistic theory. But being statistical doesn't justify the absence of interpretability. I believe that statistical language models can totally be interpretable, as long as they are built gradually within the modeling-understanding cycle.

In future posts, I will talk about some of my own research in computational linguistics. Some of the topics are: similarity based generalization, word sense induction, distributional composition, grammar induction, etc. 
